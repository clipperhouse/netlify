---
title: "Epistemology and human factors: deal with ‘em"
date: 2013-06-20
author: "Matt Sherman"

---

[This study](http://www.nytimes.com/2013/06/20/business/in-head-hunting-big-data-may-not-be-such-a-big-deal.html?pagewanted=all) that Google conducted to find predictors for success in their hiring process is very interesting. Among the things that are not predictive of success: the candidate’s GPA, their ability to solve brain-teasers, and which manager did the interview.

The hiring process is very difficult. It’s largely intuitive and we make decisions based on personal heuristics. All of which is okay — in fact, we don’t have much of a choice, since actual predictors are hard to come by.

The dangerous bit is the belief that a process is rational, when it’s not. We are often fooled into such a belief when we have _some_ information. After all, some &gt; none.

Given [human psychology](http://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect), some information might be worse than none, because it makes us more prone to believe in our own competence.

The evidence says that belief in one’s own ability to make predictions is irrational. It’s much more rational to admit we are acting heuristically, and operate according to the fact that we don’t know much about outcomes.

This means designing systems that respond to new information, rather than making decisions and committing to them. It will emphasize judgment over rule-making, since rules are based on probably-flawed predictions of their effectiveness.

An example: we’re trying to get around Boston.

We have a map of Los Angeles. It’s _some_ information, right? It’s a map. It was created by smart professionals. It’s factual. Many people have had success with it.

So, given our lack of information about Boston, it’s only rational that we should use it. Yes?
